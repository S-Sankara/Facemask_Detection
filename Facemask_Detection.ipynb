{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising value\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading directories\n",
    "DIRECTORY = r'D:\\Face-Mask-Detection-master\\dataset'\n",
    "CATEGORIES = ['with_mask', 'without_mask']\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading datasets and converting to array\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size= (224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels into binary\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "data = np.array(data, dtype='float32')\n",
    "labels = np.array(labels)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing dataset by augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range= 20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model by MobileNet\n",
    "baseModel = MobileNetV2(weights='imagenet', include_top=False, input_tensor= Input(shape=(224,224,3)))\n",
    "\n",
    "#head of the model place on top of base model \n",
    "headModel = baseModel.output \n",
    "headModel = AveragePooling2D(pool_size=(7,7))(headModel)\n",
    "headModel = Flatten(name='flattern')(headModel)\n",
    "headModel = Dense(128, activation='relu')(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation='softmax')(headModel)\n",
    "\n",
    "model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "\n",
    "#freezing the baseModel so that weight will not updated \n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training\n",
    "opt = Adam(lr = INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=BS),\n",
    "             steps_per_epoch= len(trainX) // BS,\n",
    "             validation_data=(testX, testY),\n",
    "             validation_steps=len(testX) // BS,\n",
    "             epochs = EPOCHS,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the model\n",
    "predIdxs = model.predict(testX, batch_size = BS)\n",
    "\n",
    "# each testing image finding largest index label probability \n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs, target_names = lb.classes_))\n",
    "\n",
    "model.save('mask_detector.model', save_format='h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-production",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ploting accuracy\n",
    "plt.figure(0)\n",
    "plt.plot(H.history['acc'], label='training accuracy')\n",
    "plt.plot(H.history['val_acc'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting loss\n",
    "plt.plot(H.history['loss'], label= 'training loss')\n",
    "plt.plot(H.history['val_loss'], label= 'val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "canadian-structure",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules for testing\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import time \n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for detecting mask\n",
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "    (h,w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (224,224), (104.0, 177.0, 123.0))\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "    print(detections.shape)\n",
    "    \n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0,0,i,2]\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            box = detections[0,0,i,3:7] * np.array([w,h,w,h])\n",
    "            (startX, startY, endX, endY) = box.astype('int')\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "            \n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face = cv2.resize(face, (224, 224))\n",
    "            face = img_to_array(face)\n",
    "            face = preprocess_input(face)\n",
    "            \n",
    "            faces.append(face)\n",
    "            locs.append((startX,startY, endX,endY))\n",
    "            \n",
    "    if len(faces):\n",
    "        faces = np.array(faces, dtype='float32')\n",
    "        preds = maskNet.predict(faces, batch_size= 32)\n",
    "        \n",
    "    return (locs,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading facenet\n",
    "prototxtPath = r'D:\\Face-Mask-Detection-master\\face_detector\\deploy.prototxt'\n",
    "weightsPath = r'D:\\Face-Mask-Detection-master\\face_detector\\res10_300x300_ssd_iter_140000.caffemodel'\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "#loading model \n",
    "maskNet = load_model('mask_detector.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for video capturing\n",
    "cap = cv2.VideoCapture('Mask_test2.mp4')\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "    \n",
    "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "    \n",
    "    for (box,pred) in zip(locs,preds):\n",
    "        \n",
    "        (startX, startY, endX, endY) = box\n",
    "        (mask, withoutMask) = pred \n",
    "        \n",
    "        label = 'Mask' if mask > withoutMask else 'No Mask'\n",
    "        color = (0,255,0) if label == 'Mask' else (0,0,255)\n",
    "        \n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask)* 100)\n",
    "        \n",
    "        cv2.putText(frame, label, (startX,startY-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (startX,startY), (endX,endY), color, 2)\n",
    "        \n",
    "    cv2.imshow('Frame', frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-gnome",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
